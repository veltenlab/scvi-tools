===========
totalVI
===========

**totalVI** [#ref1]_ posits a flexible generative model of CITE-seq RNA and protein data that can subsequently
be used for many common downstream tasks.

The advantages of totalVI are:

    + Comprehensive in capabilities.

    + Scalable to very large datasets (>1 million cells).

The disadvantages of totalVI include:

    + Effectively requires a GPU for fast inference.

    + Difficult to understand the balance between RNA and protein data in the low-dimensional representation of cells.

.. topic:: Tutorials:

 - :doc:`/tutorials/notebooks/totalVI`
 - :doc:`/tutorials/notebooks/cite_scrna_integration_w_totalVI`
 - :doc:`/tutorials/notebooks/scarches_scvi_tools`


Preliminaries
==============
totalVI takes as input a scRNA-seq gene expression matrix of UMI counts :math:`X` with :math:`N` cells and :math:`G` genes
along with a paired matrix of protein abundance (UMI counts) :math:`Y`, also of :math:`N` cells, but with :math:`T` proteins.
Thus, for each cell, we observe both RNA and protein information.
Additionally, a design matrix :math:`S` containing :math:`p` observed covariates, such as day, donor, etc, is an optional input.
While :math:`S` can include both categorical covariates and continuous covariates, in the following, we assume it contains only one
categorical covariate with :math:`B` categories, which represents the common case of having multiple batches of data.


Generative process
========================

.. figure:: figures/totalvi_graphical_model.svg
   :class: img-fluid
   :align: center
   :alt: totalVI graphical model

First we define the neural networks used in generative process:

.. math::
   :nowrap:

   \begin{align}
      f_\rho(z_n, s_n) &: \Delta^{K-1} \times \{0, 1\}^B \to \Delta^{G-1}   \tag{1} \\
      g_\alpha(z_n, s_n) &: \Delta^{K-1} \times \{0, 1\}^B \to [1, \infty)^T \tag{2}\\
      h_\pi(z_n, s_n) &: \Delta^{K-1} \times \{0, 1\}^B \to (0, 1)^T \tag{3}
   \end{align}

We also have global parameters :math:`\theta_g` and :math:`\phi_t`, which represent
gene- and protein-specific (respectively) overdispersion.
We posit each cell's protein and RNA expression to be generated by the following process:

First, for each cell :math:`n`,

.. math::
   :nowrap:

   \begin{align}
      z_n &\sim \textrm{Normal}(0, I)   \tag{4} \\
      \rho_{n} &= f_\rho(z_n, s_n)  \tag{5} \\
      \alpha_n &= g_\alpha(z_n, s_n)  \tag{6} \\
      \pi_n &= h_\pi(z_n, s_n)  \tag{7} \\
      l_n &\sim \textrm{LogNormal}(l_\mu^\top s_n, l_{\sigma^2}^\top s_n) \tag{8}\\
   \end{align}

with the interpretation of each variable being

.. list-table::
   :widths: 20 90 15
   :header-rows: 1

   * - Latent variable
     - Description
     - Code variable (if different)
   * - :math:`z_n \in \mathbb{R}^d`
     - Low-dimensional representation capturing joint state of a cell
     - N/A
   * - :math:`\rho_n \in \Delta^{G-1}`
     - Denoised/normalized gene expression,
     - ``px_["scale"]``
   * - :math:`\alpha_n \in [1, \infty)^T`
     - Foreground scaling factor for proteins, identifies the mixture distribution (see below)
     - ``py_["rate_fore"]``
   * - :math:`\pi_n \in (0, 1)^T`
     - Probability of background for each protein
     - ``py_["mixing"]`` (logits scale).
   * - :math:`l_n \in (0, \infty)`
     - Library size for RNA. Here it is modeled as a latent variable, but the recent default for totalVI is to treat library size as observed, equal to the total RNA UMI count of a cell. This can be controlled by passing ``use_observed_lib_size=False`` to :class:`~scvi.model.TOTALVI`.
     - N/A

Then for each gene :math:`g` in cell :math:`n`,

.. math::
   :nowrap:

   \begin{align}
      x_{ng} &\sim \textrm{NegativeBinomial}\left(l_n\rho_{ng}, \theta_g \right), \tag{10}\\
   \end{align}

where the negative binomial is parameterized by its mean and inverse dispersion.
And finally for each protein :math:`t` in cell :math:`n`,

.. math::
    :nowrap:

    \begin{align}
       \beta_{nt} &\sim \textrm{LogNormal}(c_t^\top s_n, d_t^\top s_n)  \tag{11}\\
       v_{nt} &\sim \textrm{Bernoulli}(\pi_{nt})  \tag{12}\\
       y_{nt} &\sim \textrm{NegativeBinomial}\left(v_{nt}\beta_{nt} + (1-v_{nt})\beta_{nt}\alpha_{nt}, \phi_t \right)  \tag{14}
    \end{align}

Integrating out :math:`v_{nt}` yields a negative binomial mixture conditional distribution for :math:`y_{nt}`.
Furthermore, :math:`\beta_{nt}` represents background protein signal due to ambient antibodies or non-specific antibody binding.
The prior parameters :math:`c_t` and :math:`d_t` are unfortunately called ``background_pro_alpha`` and ``background_pro_log_beta`` in the code.
They are learned parameters during infererence, but are initialized through a procedure that fits a two-component Gaussian mixture model for each cell
and records the mean and variance of the component with smaller mean, aggregating across all cells. This can be disabled by setting ``empirical_protein_background_prior=False``,
which then forces a random Initialization.

Inference
==========

totalVI uses variational inference, and specifically auto-encoding variational bayes [#ref2]_, to learn both the model parameters (the
neural network params, dispersion params), and an approximate posterior distribution with the following factorization:

 .. math::
    :nowrap:

    \begin{align}
       q_\eta(\beta_n, z_n, l_n \mid x_n, y_n, s_n) :=
       q_\eta(\beta_n \mid z_n,s_n)q_\eta(z_n \mid x_n, y_n,s_n)q_\eta(l_n \mid x_n, y_n, s_n).
    \end{align}

Here :math:`\eta` is a set of parameters corresponding to inference neural networks, which we do not describe in detail here,
but are described in the totalVI paper [#ref1]_. totalVI can also handle missing proteins (i.e., a dataset comprised of
multiple batches, where each batch potentially has a different antibody panel, or no protein data at all).
We refer the reader to the original publication for these details.

Tasks
=====

Dimensionality reduction
-------------------------
For dimensionality reduction, we by default return the mean of the approximate posterior :math:`q_\eta(z_n \mid C_n)`.
This is achieved using the method::

    >>> latent = model.get_latent_representation()
    >>> adata.obsm["X_totalvi"] = latent

Users may also return samples from this distribution, as opposed to the mean by passing the argument `give_mean=False`.
The latent representation can be used to create a nearest neighbor graph with scanpy with::

    >>> import scanpy as sc
    >>> sc.pp.neighbors(adata, use_rep="X_totalvi")
    >>> adata.obsp["distances"]


Normalization and denoising of expression
------------------------------------------

Differential expression
-----------------------

Data simulation
---------------



.. topic:: References:

   .. [#ref1] Adam Gayoso*, ZoÃ« Steier*, Romain Lopez, Jeffrey Regier, Kristopher L Nazor, Aaron Streets, Nir Yosef (2021),
        *Joint probabilistic modeling of single-cell multi-omic data with totalVI*,
        `Nature Methods <https://www.nature.com/articles/s41592-020-01050-x>`__.
   .. [#ref2] Kingma, D. P. & Welling, M. *Auto-Encoding variational Bayes* in International Conference on Learning Representations (2014).

